{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/liujiaqi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://github.com/312shan/Subject-and-Sentiment-Analysis\n",
    "\"\"\"\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "from capsule import *\n",
    "import jieba\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, SpatialDropout2D, Activation, Embedding, Flatten, Conv2D, MaxPool2D\n",
    " \n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Reshape, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, RMSprop, SGD, Nadam\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from keras.layers import BatchNormalization\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "test_size = 0#.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.905 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22452\n",
      "Found 635793 word vectors.\n"
     ]
    }
   ],
   "source": [
    "if test_size == 0:\n",
    "    stop_words = load_stop_words()\n",
    "    seqs, seqs_dev, word2index, y_train = raw_file2matrix(train_file, test_file, stop_words)\n",
    "    embeddings_index = load_embeddings_index()\n",
    "    embedding_matrix = get_embedding_matrix(word2index, embeddings_index)  # word-index-embedding是它们之间的链接关系\n",
    "    \n",
    "    X_train = np.loadtxt('../data/output/matrixes/X_train_2')\n",
    "    y_train = np.loadtxt('../data/output/matrixes/y_train_2')\n",
    "    X_test = np.loadtxt('../data/output/matrixes/X_test_2')\n",
    "    X_trains, y_trains = generate_shuffle_array(X_train, y_train)\n",
    "    drop_array(X_trains)\n",
    "else:\n",
    "    stop_words = load_stop_words()\n",
    "    seqs_train, seqs_valid, seqs_dev, word2index, y_train, y_valid, train_id_label_dict, valid_label = raw_file_2_matrix(train_file, test_file, stop_words, test_size=test_size)\n",
    "\n",
    "    embeddings_index = load_embeddings_index()\n",
    "    embedding_matrix = get_embedding_matrix(word2index, embeddings_index)  # word-index-embedding是它们之间的链接关系\n",
    "    X_train, X_valid, X_test = get_padding_data(seqs_train, seqs_valid, seqs_dev)  # seqs needs to be a list of a list.把列表变成矩阵，列数是embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_text_cnn_model():\\n    drop = 0.5\\n    learning_rate = 0.005  # 0.0001\\n    maxlen = 100\\n    inputs = Input(shape=(maxlen,))\\n    embed_layer = Embedding(len(word2index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], \\n                            input_length=maxlen, trainable=False)(inputs)\\n    embed_layer = SpatialDropout1D(drop)(embed_layer)\\n    \\n \\n    capsule1 = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings, # kernel_size=(3, 1),\\n                      share_weights=True)(embed_layer)\\n    bn1 = BatchNormalization()(capsule1)\\n                                          \\n       \\n    capsule2 = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings, # kernel_size=(3, 1),\\n                      share_weights=True)(embed_layer)\\n    bn2 = BatchNormalization()(capsule2)\\n                                      \\n    bn = Concatenate(axis=1)([bn1, bn2])\\n    bn = Flatten()(bn)\\n\\n    fc = Dense(300)(bn)\\n    bn = BatchNormalization()(fc)\\n    bn = Activation(\\'relu\\')(bn)\\n    bn_dropout = Dropout(drop)(bn)\\n    outputs = Dense(30, activation=\"sigmoid\")(bn_dropout)\\n    print(outputs.shape)\\n    model = Model(inputs=inputs, outputs=outputs)\\n\\n    # model.compile(loss=\\'categorical_crossentropy\\', optimizer=sgd, metrics=[\\'accuracy\\'])\\n    model.compile(loss=\\'binary_crossentropy\\', optimizer=\\'adam\\', metrics=[f1_score])\\n\\n    return model\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def get_text_cnn_model():\n",
    "    drop = 0.5\n",
    "    learning_rate = 0.005  # 0.0001\n",
    "    maxlen = 100\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    embed_layer = Embedding(len(word2index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                            input_length=maxlen, trainable=False)(inputs)\n",
    "    embed_layer = SpatialDropout1D(drop)(embed_layer)\n",
    "    \n",
    " \n",
    "    capsule1 = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings, # kernel_size=(3, 1),\n",
    "                      share_weights=True)(embed_layer)\n",
    "    bn1 = BatchNormalization()(capsule1)\n",
    "                                          \n",
    "       \n",
    "    capsule2 = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings, # kernel_size=(3, 1),\n",
    "                      share_weights=True)(embed_layer)\n",
    "    bn2 = BatchNormalization()(capsule2)\n",
    "                                      \n",
    "    bn = Concatenate(axis=1)([bn1, bn2])\n",
    "    bn = Flatten()(bn)\n",
    "\n",
    "    fc = Dense(300)(bn)\n",
    "    bn = BatchNormalization()(fc)\n",
    "    bn = Activation('relu')(bn)\n",
    "    bn_dropout = Dropout(drop)(bn)\n",
    "    outputs = Dense(30, activation=\"sigmoid\")(bn_dropout)\n",
    "    print(outputs.shape)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_score])\n",
    "\n",
    "    return model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cnn():  # conv 中的acativation???\n",
    "    maxlen = 100\n",
    "    num_filter = 256\n",
    "    drop = 0.55\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    \n",
    "    embed_layer = Embedding(len(word2index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                            input_length=maxlen, trainable=True)(inputs)\n",
    "    embed_layer = SpatialDropout1D(drop)(embed_layer)\n",
    "    conv1 = Conv1D(filters=num_filter, kernel_size=1, strides=1, padding='same',activation=None)(embed_layer)\n",
    "    # conv1 = Conv2D(filters=num_filter, kernel_size=(1,EMBEDDING_DIM), strides=(1, 1), padding='valid', activation='relu')(embed_layer)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv1D(filters=num_filter, kernel_size=1, strides=1, padding='same', activation=None)(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1_pool = MaxPooling1D(pool_size=2, strides=None, padding='same')(conv1)\n",
    "    \n",
    "\n",
    "    conv2 = Conv1D(filters=num_filter, kernel_size=2, strides=1, padding='same', activation=None)(embed_layer)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv1D(filters=num_filter, kernel_size=2, strides=1, padding='same', activation=None)(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2_pool = MaxPooling1D(pool_size=2, strides=None, padding='same')(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters=num_filter, kernel_size=3, strides=1, padding='same', activation=None)(embed_layer)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv1 = Activation('relu')(conv3)\n",
    "    conv3 = Conv1D(filters=num_filter, kernel_size=3, strides=1, padding='same', activation=None)(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3_pool = MaxPooling1D(pool_size=2, strides=None, padding='same')(conv3)\n",
    "    \n",
    "    conv4 = Conv1D(filters=num_filter, kernel_size=4, strides=1, padding='same', activation=None)(embed_layer)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4 = Conv1D(filters=num_filter, kernel_size=4, strides=1, padding='same', activation=None)(conv4)\n",
    "    conv4 = BatchNormalization()(conv1)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4_pool = MaxPooling1D(pool_size=2, strides=None, padding='same')(conv4)\n",
    "    \n",
    "    conv5 = Conv1D(filters=num_filter, kernel_size=5, strides=1, padding='same', activation=None)(embed_layer)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv1D(filters=num_filter, kernel_size=5, strides=1, padding='same', activation=None)(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5_pool = MaxPooling1D(pool_size=2, strides=None, padding='same')(conv5)\n",
    "    \n",
    "    conv_concat = Concatenate(axis=1)([conv1_pool, conv2_pool, conv3_pool, conv4_pool, conv5_pool])\n",
    "    \n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(conv_concat)\n",
    "    max_pool = GlobalMaxPooling1D()(conv_concat)\n",
    "    conv_concat = Concatenate()([avg_pool, max_pool])\n",
    "    # conv_concat = Flatten()(conv_concat)\n",
    "    conv_concat = Dropout(drop)(conv_concat)\n",
    "    conv_concat = Dense(300)(conv_concat)\n",
    "    conv_bn = BatchNormalization()(conv_concat)\n",
    "    conv_relu = Activation('relu')(conv_bn)\n",
    "    # conv_dropout = Dropout(drop)(conv_relu)\n",
    "    outputs = Dense(30, activation='sigmoid')(conv_relu)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    learning_rate = 0.01\n",
    "    adam = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.00001)\n",
    "    sgd = SGD(lr=learning_rate, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    rmsprop = RMSprop(lr=learning_rate, rho=0.9, epsilon=None, decay=0.0)\n",
    "    nadam = keras.optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[f1_score])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29828 samples, validate on 12784 samples\n",
      "Epoch 1/30\n",
      "29828/29828 [==============================] - 276s 9ms/step - loss: 0.1154 - f1_score: 0.3645 - val_loss: 0.0843 - val_f1_score: 0.5291\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.52915, saving model to ../models/weights/text_cnn_weights-dataArg-01-0.5291-0.0843.hdf5\n",
      "Epoch 2/30\n",
      "29828/29828 [==============================] - 462s 15ms/step - loss: 0.0940 - f1_score: 0.4649 - val_loss: 0.0819 - val_f1_score: 0.5085\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.52915\n",
      "Epoch 3/30\n",
      "29828/29828 [==============================] - 432s 14ms/step - loss: 0.0905 - f1_score: 0.4804 - val_loss: 0.0787 - val_f1_score: 0.5719\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.52915 to 0.57189, saving model to ../models/weights/text_cnn_weights-dataArg-03-0.5719-0.0787.hdf5\n",
      "Epoch 4/30\n",
      "29828/29828 [==============================] - 437s 15ms/step - loss: 0.0880 - f1_score: 0.4947 - val_loss: 0.0756 - val_f1_score: 0.5654\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.57189\n",
      "Epoch 5/30\n",
      "29828/29828 [==============================] - 464s 16ms/step - loss: 0.0858 - f1_score: 0.5001 - val_loss: 0.0733 - val_f1_score: 0.5742\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.57189 to 0.57416, saving model to ../models/weights/text_cnn_weights-dataArg-05-0.5742-0.0733.hdf5\n",
      "Epoch 6/30\n",
      "27712/29828 [==========================>...] - ETA: 30s - loss: 0.0835 - f1_score: 0.5159"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    # log_filepath = './tmp/log'\n",
    "    filepath=\"../models/weights/text_cnn_weights-dataArg-{epoch:02d}-{val_f1_score:.4f}-{val_loss:.4f}.hdf5\"\n",
    "    # callback = keras.callbacks.TensorBoard(log_dir=log_filepath, write_images=1, histogram_freq=1)\n",
    "    \n",
    "    # early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_f1_score', verbose=1, save_best_only=True, \n",
    "                                 save_weights_only=True, mode='max')\n",
    "    model = text_cnn()\n",
    "    # model.load_weights('../models/weights/text_cnn_weights-filter-64-12-0.5965-0.0780.hdf5')\n",
    "    model.fit(X_trains, y_trains, batch_size=16, shuffle=True, epochs=30, verbose=1, \n",
    "              callbacks=[checkpoint], # , callback], \n",
    "              validation_split=0.33) # batch_size: 16, epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = text_cnn()\n",
    "model.load_weights('../models/weights/text_cnn_weights-filter-256-10-0.6441-0.0643.hdf5')\n",
    "pred4  = model.predict(X_test, batch_size=1024)\n",
    "res, res_df = pred2res(pred4)\n",
    "res_df.head(50)\n",
    "np.savetxt('../data/output/text_cnn_0.6441_1106.txt',pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# multi = np.loadtxt('../data/output/multi_6405_6378.txt')\n",
    "hi = np.loadtxt('../data/output/hi_bi_gru_6476_1106.txt')\n",
    "text = np.loadtxt('../data/output/text_cnn_0.6441_1106.txt')\n",
    "capsule = np.loadtxt('../data/output/capsule_6454_1106.txt')\n",
    "bi_gru_attention = np.loadtxt('../data/output/bi_gru_6376_1101(best).txt')\n",
    "incep = np.loadtxt('../data/output/text_cnn_0.6099_1106.txt')\n",
    "hi2 = np.loadtxt('../data/output/hi_bi_gru_6441_1101-best.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = bi_gru_attention *0.2+hi*0.3+text*0.5\n",
    "pred2 = capsule *0.25+text*0.25+hi*0.25+bi_gru_attention*0.25\n",
    "pred3 = incep*0.2+hi2*0.3+text*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, res_df = pred2res(pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('../data/output/submission/inc_hi_text_235.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
