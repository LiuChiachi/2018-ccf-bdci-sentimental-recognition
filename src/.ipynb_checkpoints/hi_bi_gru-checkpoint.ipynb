{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/liujiaqi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://github.com/312shan/Subject-and-Sentiment-Analysis\n",
    "\"\"\"\n",
    "import keras \n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from capsule import *\n",
    "from utils import *\n",
    "import jieba\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalMaxPooling1D, MaxPooling1D, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Dropout, Activation, SpatialDropout1D\n",
    "from keras.layers import Input, Bidirectional, RNN, Concatenate,  Flatten\n",
    "from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.layers import BatchNormalization, Permute\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "train_file = '../data/input/train_2.csv'\n",
    "test_file  = '../data/input/test_public_2.csv'\n",
    " \n",
    "test_size= 0# .33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.892 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22452\n",
      "Found 635793 word vectors.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e9e9fe4a7638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/output/matrixes/y_train_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/output/matrixes/X_test_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mX_trains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_shuffle_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdrop_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sentiment_recognition/src/utils.py\u001b[0m in \u001b[0;36mgenerate_shuffle_array\u001b[0;34m(X_train, num)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_shuffle_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mX_trains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0my_trains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "if test_size == 0:\n",
    "    stop_words = load_stop_words()\n",
    "\n",
    "    seqs, seqs_dev, word2index, y_train = raw_file2matrix(train_file, test_file, stop_words)\n",
    "    \n",
    "    embeddings_index = load_embeddings_index()\n",
    "    embedding_matrix = get_embedding_matrix(word2index, embeddings_index)  # word-index-embedding是它们之间的链接关系\n",
    "    X_train = np.loadtxt('../data/output/matrixes/X_train_2')\n",
    "    y_train = np.loadtxt('../data/output/matrixes/y_train_2')\n",
    "    X_test = np.loadtxt('../data/output/matrixes/X_test_2')\n",
    "    X_trains, y_trains = generate_shuffle_array(X_train, y_train)\n",
    "    drop_array(X_trains)\n",
    "else:\n",
    "    stop_words = load_stop_words()\n",
    "    seqs_train, seqs_valid, seqs_dev, word2index, y_train, y_valid, train_id_label_dict, valid_label = raw_file_2_matrix(train_file, test_file, stop_words, test_size=test_size)\n",
    "\n",
    "    embeddings_index = load_embeddings_index()\n",
    "    embedding_matrix = get_embedding_matrix(word2index, embeddings_index)  # word-index-embedding是它们之间的链接关系\n",
    "    X_train, X_valid, X_test = get_padding_data(seqs_train, seqs_valid, seqs_dev)  # seqs needs to be a list of a list.把列表变成矩阵，列数是embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    # https://github.com/keras-team/keras/issues/1472\n",
    "    # https://github.com/philipperemy/keras-attention-mechanism\n",
    "    TIME_STEPS = 20\n",
    "    INPUT_DIM = 2\n",
    "    # if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
    "    SINGLE_ATTENTION_VECTOR = False\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    # print(a.shape, input_dim, TIME_STEPS)\n",
    "    # a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    # a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    # print(inputs.shape, a_probs.shape)\n",
    "    # output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    output_attention_mul = Concatenate(axis=1)([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "    \"\"\"\n",
    "    # https://stackoverflow.com/questions/42918446/how-to-add-an-attention-mechanism-in-keras\n",
    "    activations = LSTM(units, return_sequences=True)(embedded)\n",
    "\n",
    "    # compute importance for each step\n",
    "    attention = Dense(1, activation='tanh')(activations)\n",
    "    attention = Flatten()(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = RepeatVector(units)(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "\n",
    "    sent_representation = merge([activations, attention], mode='mul')\n",
    "    \"\"\"\n",
    "def model_attention_applied_after_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    lstm_units = 32\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "    \n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    # 3个串联  0.10\n",
    "    drop = 0.55# 0.55\n",
    "    # dropout_p = 0.5\n",
    "    learning_rate = 0.001  # 0.0001\n",
    "    gru_units= 128 # 100\n",
    "    maxlen = 100\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    embed_layer = Embedding(len(word2index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                            input_length=maxlen, trainable=True)(inputs)\n",
    "    embed_layer = SpatialDropout1D(drop)(embed_layer)\n",
    "    \"\"\"x = LSTM(output_dim=100,activation='relu',inner_activation='relu', return_sequences=True)(x)\"\"\"\n",
    "    x1 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                          return_sequences=True))(embed_layer)\n",
    "    x1 = attention_3d_block(x1)\n",
    "    x2 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                      return_sequences=True))(embed_layer)\n",
    "    x2 = attention_3d_block(x2)\n",
    "    x3= Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                      return_sequences=True))(x2)\n",
    "    x3 = attention_3d_block(x3)\n",
    "    \"\"\"\n",
    "    x3 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                       return_sequences=True))(x)\n",
    "    x4 = Concatenate(axis=1)([x1, x2, x3])\n",
    "    # x = Dense(200, activation='relu')(x)\n",
    "    x4 = Dropout(drop)(x4)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x5 = Concatenate(axis=1)([avg_pool, max_pool])\n",
    "    \"\"\"\n",
    "    x4 = Concatenate(axis=1)([x1, x3])\n",
    "    avg_pool = GlobalAveragePooling1D()(x4)\n",
    "    max_pool = GlobalMaxPooling1D()(x4)\n",
    "    x5 = Concatenate(axis=1)([avg_pool, max_pool])\n",
    "    fc = Dense(300)(x5)\n",
    "    bn = BatchNormalization()(fc)\n",
    "    bn = Activation('relu')(bn)\n",
    "    bn_dropout = Dropout(drop)(bn)\n",
    "    # bn_dropout = Flatten()(bn_dropout)\n",
    "    outputs = Dense(30, activation='sigmoid')(bn_dropout)\n",
    "    print(outputs.shape)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    adam = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n",
    "    sgd = SGD(lr=learning_rate, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    rmsprop = RMSprop(lr=learning_rate, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[f1_score])\n",
    "    # model.fit(X_train, y_train, batch_size=16, nb_epoch=1, validation_split=0.1, shuffle=True)\n",
    "              # validation_data=(p_X_test, p_y_test))\n",
    "    # score, acc = model.evaluate(p_X_test, p_y_test, batch_size=batch_size)\n",
    "    #  model.load_weights(fname, by_name=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_model_results = []\n",
    "for i in range(5):\n",
    "    # log_filepath = './tmp/log/hi_bi_gru' \n",
    "    # callback = [keras.callbacks.TensorBoard(log_dir=log_filepath, write_images=1, histogram_freq=1)] \n",
    "    filepath=\"../models/weights/hi_bi_gru-attention-adataArg-{epoch:02d}-{val_f1_score:.4f}-{val_loss:.4f}.hdf5\"\n",
    "    # checkpoint = ModelCheckpoint(filepath, monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\n",
    "    model = get_model()\n",
    "    # model.load_weights('../models/weights/best-hi_bi_gru-improvement-30-0.6441-0.0691.hdf5')\n",
    "    model.fit(X_trains, y_trains, batch_size=16, epochs=40, shuffle=True,\n",
    "              callbacks=[checkpoint], \n",
    "              validation_split=0.3)  # batch_size: 16, epochs = 40\n",
    "    # first_model_results.append(model.predict(X_test, batch_size=1024))\n",
    "# pred4 = np.average(first_model_results, axis=0)\n",
    "# f1_score = get_f1_score(valid_label, res) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 30)\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.load_weights('../models/weights/hi_bi_gru-attention-units-128-14-0.6476-0.0623.hdf5')\n",
    "pred4 = model.predict(X_test, batch_size=1024)\n",
    "np.savetxt('../data/output/hi_bi_gru_6476_1106.txt', pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = np.loadtxt('../data/output/text_cnn_pred_0.6381.txt')\n",
    "pred2 = np.loadtxt('../data/output/cap_pred_0.6256.txt')\n",
    "pred4 = np.loadtxt\n",
    "pred4 = pred4*0.4+pred1*0.4+pred2*0.2\n",
    "res, res_df = pred2res(pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('../data/output/submission/bagging.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
