{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/liujiaqi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"https://github.com/312shan/Subject-and-Sentiment-Analysis\"\"\"\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "from capsule import *\n",
    "import jieba\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalMaxPooling1D, MaxPooling1D, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Dropout, Activation, SpatialDropout1D\n",
    "from keras.layers import Input, Bidirectional, RNN, Concatenate,  Flatten\n",
    "from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.layers import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from keras.layers import Permute, Reshape, merge\n",
    "K.clear_session()\n",
    "import keras \n",
    "\n",
    "train_file = '../data/input/train_2.csv'\n",
    "test_file  = '../data/input/test_public_2.csv'\n",
    "test_size = 0# .25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.224 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22452\n",
      "Found 635793 word vectors.\n"
     ]
    }
   ],
   "source": [
    "if test_size == 0:\n",
    "    stop_words = load_stop_words()\n",
    "    seqs, seqs_dev, word2index, y_train = raw_file2matrix(train_file, test_file, stop_words)\n",
    "    embeddings_index = load_embeddings_index()\n",
    "    embedding_matrix = get_embedding_matrix(word2index, embeddings_index)  # word-index-embedding是它们之间的链接关系\n",
    "    \n",
    "    X_train = np.loadtxt('../data/output/matrixes/X_train_2')\n",
    "    y_train = np.loadtxt('../data/output/matrixes/y_train_2')\n",
    "    X_test = np.loadtxt('../data/output/matrixes/X_test_2')\n",
    "\n",
    "else:\n",
    "    stop_words = load_stop_words()\n",
    "    seqs_train, seqs_valid, seqs_dev, word2index, y_train, y_valid, train_id_label_dict, valid_label = raw_file_2_matrix(train_file, test_file, stop_words, test_size=test_size)\n",
    "    embeddings_index = load_embeddings_index()\n",
    "    embedding_matrix = get_embedding_matrix(word2index, embeddings_index)  # word-index-embedding是它们之间的链接关系\n",
    "    X_train, X_valid, X_test = get_padding_data(seqs_train, seqs_valid, seqs_dev)  # seqs needs to be a list of a list.把列表变成矩阵，列数是embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    # https://github.com/keras-team/keras/issues/1472\n",
    "    # https://github.com/philipperemy/keras-attention-mechanism\n",
    "    TIME_STEPS = 20\n",
    "    INPUT_DIM = 2\n",
    "    # if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
    "    SINGLE_ATTENTION_VECTOR = False\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    # print(a.shape, input_dim, TIME_STEPS)\n",
    "    # a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    # a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    # print(inputs.shape, a_probs.shape)\n",
    "    # output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    output_attention_mul = Concatenate(axis=1)([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "    \"\"\"\n",
    "    # https://stackoverflow.com/questions/42918446/how-to-add-an-attention-mechanism-in-keras\n",
    "    activations = LSTM(units, return_sequences=True)(embedded)\n",
    "\n",
    "    # compute importance for each step\n",
    "    attention = Dense(1, activation='tanh')(activations)\n",
    "    attention = Flatten()(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = RepeatVector(units)(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "\n",
    "    sent_representation = merge([activations, attention], mode='mul')\n",
    "    \"\"\"\n",
    "def model_attention_applied_after_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    lstm_units = 32\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "    \n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
    "    \"\"\"\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "def get_bi_gru_model():\n",
    "    # 3个串联  0.10\n",
    "    drop = 0.60# 0.55\n",
    "\n",
    "    gru_units= 128  # 100\n",
    "    maxlen = 100\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    embed_layer = Embedding(len(word2index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                            input_length=maxlen, trainable=True)(inputs)\n",
    "    embed_layer = SpatialDropout1D(drop)(embed_layer)\n",
    "    x1 = Bidirectional(LSTM(gru_units, activation='relu', dropout=drop, recurrent_dropout=drop, \n",
    "                            return_sequences=True))(embed_layer)\n",
    "    x1 = attention_3d_block(x1)\n",
    "    x2 = Bidirectional(LSTM(gru_units, activation='relu', dropout=drop, recurrent_dropout=drop, \n",
    "                            return_sequences=True))(embed_layer)\n",
    "    x2 = attention_3d_block(x2)\n",
    "    x3 = Concatenate(axis=1)([x1, x2])\n",
    "    avg_pool = GlobalAveragePooling1D()(x3)\n",
    "    max_pool = GlobalMaxPooling1D()(x3)\n",
    "    # print(avg_pool.shape, max_pool.shape)\n",
    "    x5 = Concatenate(axis=1)([avg_pool, max_pool])\n",
    "    print(x5.shape)\n",
    "    fc = Dense(300)(x5)\n",
    "    bn = BatchNormalization()(fc)\n",
    "    bn = Activation('relu')(bn)\n",
    "    bn_dropout = Dropout(drop)(bn)\n",
    "    # bn_dropout = Flatten()(bn_dropout)\n",
    "    outputs = Dense(30, activation='sigmoid')(bn_dropout)\n",
    "    # print(outputs.shape)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    learning_rate = 0.01  # 0.0001\n",
    "    adam = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0)\n",
    "    sgd = SGD(lr=learning_rate, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    rmsprop = RMSprop(lr=learning_rate, rho=0.9, epsilon=None, decay=0.0)\n",
    "    nadam = keras.optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_score])\n",
    "    # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_score])\n",
    "    # model.fit(X_train, y_train, batch_size=16, nb_epoch=1, validation_split=0.1, shuffle=True)\n",
    "              # validation_data=(p_X_test, p_y_test))\n",
    "    # score, acc = model.evaluate(p_X_test, p_y_test, batch_size=batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7457 samples, validate on 3196 samples\n",
      "Epoch 1/30\n",
      "7457/7457 [==============================] - 256s 34ms/step - loss: nan - f1_score: nan - val_loss: nan - val_f1_score: nan\n",
      "\n",
      "Epoch 00001: val_f1_score did not improve from -inf\n",
      "Epoch 2/30\n",
      "2880/7457 [==========>...................] - ETA: 2:18 - loss: nan - f1_score: nan"
     ]
    }
   ],
   "source": [
    "# first_model_results = []\n",
    "for i in range(5):\n",
    "    model = get_bi_gru_model()\n",
    "    \n",
    "\n",
    "    # log_filepath = './tmp/log' \n",
    "    filepath=\"../models/weights/bi_lstm_attention-{epoch:02d}-{val_f1_score:.4f}-{val_loss:.4f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_f1_score', verbose=1, save_best_only=True,\n",
    "    mode='max')\n",
    "    # callback = [keras.callbacks.TensorBoard(log_dir=log_filepath, write_images=1, histogram_freq=1) ]\n",
    "    model.fit(X_train, y_train, batch_size=16, epochs=30, shuffle=True, callbacks=[checkpoint], \n",
    "                        validation_split=0.3)  # batch_size: 16, epochs = 40\n",
    "    \n",
    "    # first_model_results.append(model.predict(X_test, batch_size=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model_results = []\n",
    "model= get_bi_gru_model()\n",
    "model.load_weights('../models/weights/bi_gru_weights-improvement-15-0.6376-0.0671.hdf5')\n",
    "first_model_results.append(model.predict(X_test, batch_size=1024))\n",
    "\n",
    "model= get_bi_gru_model()\n",
    "model.load_weights('../models/weights/bi_gru_weights-improvement-15-0.6376-0.0671.hdf5')\n",
    "first_model_results.append(model.predict(X_test, batch_size=1024))\n",
    "\n",
    "np.savetxt('../data/output/bi_gru_6376_1101.txt', pred4)\n",
    "# res, res_df = pred2res(pred4)\n",
    "# res_df = np.savetxt('../data/output/bi_gru.txt', index=False)\n",
    "# f1_score = get_f1_score(valid_label, res) # 0.6223021582733813\n",
    "# res_df.to_csv('../data/output/submission/bi_gru.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= get_bi_gru_model()\n",
    "model.load_weights('../models/weights/1104-bi_gru_attention_weights-improvement-21-0.6526-0.0645.hdf5')\n",
    "pred4 = model.predict(X_test, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../data/output/bi_gru_attention_6526_1104.txt', pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
