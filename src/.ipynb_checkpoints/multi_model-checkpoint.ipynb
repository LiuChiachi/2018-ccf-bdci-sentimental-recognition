{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/liujiaqi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://github.com/312shan/Subject-and-Sentiment-Analysis\n",
    "\"\"\"\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "from capsule import *\n",
    "import jieba\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, SpatialDropout2D, Activation, Embedding, Flatten, Add\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Reshape, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from keras.layers import BatchNormalization, Permute\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "test_size = 0#.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.869 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22452\n",
      "Found 635793 word vectors.\n"
     ]
    }
   ],
   "source": [
    "if test_size == 0:\n",
    "    stop_words = load_stop_words()\n",
    "    seqs, seqs_dev, word2index, y_train = raw_file2matrix(train_file, test_file, stop_words)\n",
    "    embeddings_index = load_embeddings_index()\n",
    "    embedding_matrix = get_embedding_matrix(word2index, embeddings_index)  # word-index-embedding是它们之间的链接关系\n",
    "    \n",
    "    X_train = np.loadtxt('../data/output/matrixes/X_train_2')\n",
    "    y_train = np.loadtxt('../data/output/matrixes/y_train_2')\n",
    "    X_test = np.loadtxt('../data/output/matrixes/X_test_2')\n",
    "    X_trains, y_trains = generate_shuffle_array(X_train, y_train)\n",
    "    drop_array(X_trains)\n",
    "else:\n",
    "    stop_words = load_stop_words()\n",
    "    seqs_train, seqs_valid, seqs_dev, word2index, y_train, y_valid, train_id_label_dict, valid_label = raw_file_2_matrix(train_file, test_file, stop_words, test_size=test_size)\n",
    "\n",
    "    embeddings_index = load_embeddings_index()\n",
    "    embedding_matrix = get_embedding_matrix(word2index, embeddings_index)  # word-index-embedding是它们之间的链接关系\n",
    "    X_train, X_valid, X_test = get_padding_data(seqs_train, seqs_valid, seqs_dev)  # seqs needs to be a list of a list.把列表变成矩阵，列数是embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    # https://github.com/keras-team/keras/issues/1472\n",
    "    # https://github.com/philipperemy/keras-attention-mechanism\n",
    "    TIME_STEPS = 20\n",
    "    INPUT_DIM = 2\n",
    "    # if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
    "    SINGLE_ATTENTION_VECTOR = False\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    # print(a.shape, input_dim, TIME_STEPS)\n",
    "    # a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    # a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    # print(inputs.shape, a_probs.shape)\n",
    "    # output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    output_attention_mul = Concatenate(axis=1)([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "    \"\"\"\n",
    "    # https://stackoverflow.com/questions/42918446/how-to-add-an-attention-mechanism-in-keras\n",
    "    activations = LSTM(units, return_sequences=True)(embedded)\n",
    "\n",
    "    # compute importance for each step\n",
    "    attention = Dense(1, activation='tanh')(activations)\n",
    "    attention = Flatten()(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = RepeatVector(units)(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "\n",
    "    sent_representation = merge([activations, attention], mode='mul')\n",
    "    \"\"\"\n",
    "def model_attention_applied_after_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    lstm_units = 32\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "    \n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model\n",
    "\n",
    "def multi_model():  # conv 中的acativation???\n",
    "    maxlen = 100\n",
    "    num_filter = 128\n",
    "    drop = 0.55\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    \n",
    "    embed_layer = Embedding(len(word2index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                            input_length=maxlen, trainable=True)(inputs)\n",
    "    embed_layer = SpatialDropout1D(drop)(embed_layer)\n",
    "    \"\"\"**************************************text_cnn*********************************************************\"\"\"\n",
    "    conv1 = Conv1D(filters=num_filter, kernel_size=1, strides=1, padding='same')(embed_layer)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv1D(filters=num_filter, kernel_size=1, strides=1, padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1_pool = MaxPooling1D(pool_size=2, strides=None, padding='same')(conv1)\n",
    "   \n",
    "    conv2 = Conv1D(filters=num_filter, kernel_size=2, strides=1, padding='same')(embed_layer)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv1D(filters=num_filter, kernel_size=2, strides=1, padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2_pool = MaxPooling1D(pool_size=2, strides=None, padding='same')(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters=num_filter, kernel_size=3, strides=1, padding='same')(embed_layer)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv1 = Activation('relu')(conv3)\n",
    "    conv3 = Conv1D(filters=num_filter, kernel_size=3, strides=1, padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3_pool = MaxPooling1D(pool_size=2, strides=None, padding='valid')(conv3)\n",
    "    \n",
    "    conv4 = Conv1D(filters=num_filter, kernel_size=4, strides=1, padding='same')(embed_layer)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4 = Conv1D(filters=num_filter, kernel_size=4, strides=1, padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv1)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4_pool = MaxPooling1D(pool_size=2, strides=None, padding='same')(conv4)\n",
    "    \n",
    "    conv5 = Conv1D(filters=num_filter, kernel_size=5, strides=1, padding='same')(embed_layer)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv1D(filters=num_filter, kernel_size=5, strides=1, padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5_pool =MaxPooling1D(pool_size=2, strides=None, padding='valid')(conv5)\n",
    "    \n",
    "    conv_concat = Concatenate(axis=1)([conv1_pool, conv2_pool, conv3_pool, conv4_pool, conv5_pool])\n",
    "    # conv_concat = Flatten()(conv_concat)\n",
    "    conv_concat = Dropout(drop)(conv_concat)\n",
    "    # conv_concat = Activation('linear')(conv_concat)\n",
    "    conv_concat = Dense(300)(conv_concat)\n",
    "    conv_bn = BatchNormalization()(conv_concat)\n",
    "    conv_relu = Activation('relu')(conv_bn)\n",
    "    conv_dropout = Dropout(drop)(conv_relu)\n",
    "    outputs1 = Dense(30, activation='sigmoid', name='text_cnn')(conv_dropout)\n",
    " \n",
    "\n",
    "    \"\"\"***************************************bi_gru********************************************************\"\"\"\n",
    "    gru_units= 64 # 100\n",
    " \n",
    " \n",
    "    \"\"\"x = LSTM(output_dim=100,activation='relu',inner_activation='relu', return_sequences=True)(x)\"\"\"\n",
    "    x1 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                          return_sequences=True))(embed_layer)\n",
    "  \n",
    "    x2 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                      return_sequences=True))(embed_layer) \n",
    "    x3= Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                      return_sequences=True))(x2)\n",
    "    \"\"\"\n",
    "    x3 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                       return_sequences=True))(x)\n",
    "    x4 = Concatenate(axis=1)([x1, x2, x3])\n",
    "    # x = Dense(200, activation='relu')(x)\n",
    "    x4 = Dropout(drop)(x4)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x5 = Concatenate(axis=1)([avg_pool, max_pool])\n",
    "    \"\"\"\n",
    "    x4 = Concatenate(axis=1)([x1, x3])\n",
    "    avg_pool = GlobalAveragePooling1D()(x4)\n",
    "    max_pool = GlobalMaxPooling1D()(x4)\n",
    "    x5 = Concatenate(axis=1)([avg_pool, max_pool])\n",
    "    fc = Dense(300)(x5)\n",
    "    bn = BatchNormalization()(fc)\n",
    "    bn = Activation('relu')(bn)\n",
    "    bn_dropout = Dropout(drop)(bn)\n",
    "    # bn_dropout = Flatten()(bn_dropout)\n",
    "    outputs2 = Dense(30, activation='sigmoid', name='bi_gru')(bn_dropout)\n",
    "    \n",
    "    \"\"\"******************************************hi_bi_gru*****************************************************\"\"\"\n",
    "\n",
    "    drop = 0.55# 0.55\n",
    "    gru_units= 64 # 100\n",
    "    maxlen = 100 \n",
    "    \"\"\"x = LSTM(output_dim=100,activation='relu',inner_activation='relu', return_sequences=True)(x)\"\"\"\n",
    "    x1 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                          return_sequences=True))(embed_layer)\n",
    "  \n",
    "    x2 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                      return_sequences=True))(embed_layer) \n",
    "    x3= Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                      return_sequences=True))(x2)\n",
    "    \"\"\"\n",
    "    x3 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                       return_sequences=True))(x)\n",
    "    x4 = Concatenate(axis=1)([x1, x2, x3])\n",
    "    # x = Dense(200, activation='relu')(x)\n",
    "    x4 = Dropout(drop)(x4)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x5 = Concatenate(axis=1)([avg_pool, max_pool])\n",
    "    \"\"\"\n",
    "    x4 = Concatenate(axis=1)([x1, x3])\n",
    "    avg_pool = GlobalAveragePooling1D()(x4)\n",
    "    max_pool = GlobalMaxPooling1D()(x4)\n",
    "    x5 = Concatenate(axis=1)([avg_pool, max_pool])\n",
    "    fc = Dense(300)(x5)\n",
    "    bn = BatchNormalization()(fc)\n",
    "    bn = Activation('relu')(bn)\n",
    "    bn_dropout = Dropout(drop)(bn)\n",
    "    # bn_dropout = Flatten()(bn_dropout)\n",
    "    outputs3 = Dense(30, activation='sigmoid', name='hi_bi_gru')(bn_dropout)\n",
    "\n",
    "    \"\"\"******************************************capsule*****************************************************\"\"\"\n",
    " \n",
    "\n",
    "    x = Bidirectional(\n",
    "        GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n",
    "        embed_layer)\n",
    "    # capsule是一个卷积网络\n",
    "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n",
    "                      share_weights=True)(x)\n",
    "    # output_capsule = Lambda(lambda x: K.sqrt(K.sum(K.square(x), 2)))(capsule)\n",
    "    capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "    outputs4 = Dense(30, activation='sigmoid', name='capsule')(capsule)\n",
    "    \n",
    "    outputs = Add()([outputs1, outputs2, outputs3, outputs4])\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    learning_rate = 0.001  # 0.0001\n",
    "\n",
    "    adam = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-5)\n",
    "    sgd = SGD(lr=learning_rate, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    rmsprop = RMSprop(lr=learning_rate, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[f1_score])\n",
    "    return model\n",
    "\n",
    "\n",
    "def multi_model_independent_embedding():  # conv 中的acativation???\n",
    "    maxlen = 100\n",
    "    num_filter = 128\n",
    "    drop = 0.55\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "\n",
    "    \"\"\"**************************************text_cnn*********************************************************\"\"\"\n",
    "    \n",
    "    embed_layer1 = Embedding(len(word2index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                            input_length=maxlen, trainable=True)(inputs)\n",
    "\n",
    "    embed_layer1= SpatialDropout1D(drop)(embed_layer1)\n",
    "\n",
    "    conv1 = Conv1D(filters=num_filter, kernel_size=1, strides=1, padding='same',activation=None)(embed_layer1)\n",
    "    # conv1 = Conv2D(filters=num_filter, kernel_size=(1,EMBEDDING_DIM), strides=(1, 1), padding='valid', activation='relu')(embed_layer)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv1D(filters=num_filter, kernel_size=1, strides=1, padding='same', activation=None)(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1_pool = MaxPooling1D(pool_size=2, strides=None, padding='same')(conv1)\n",
    "    \n",
    "\n",
    "    conv2 = Conv1D(filters=num_filter, kernel_size=2, strides=1, padding='same', activation=None)(embed_layer1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv1D(filters=num_filter, kernel_size=2, strides=1, padding='same', activation=None)(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2_pool = MaxPooling1D(pool_size=2, strides=None, padding='same')(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters=num_filter, kernel_size=3, strides=1, padding='same', activation=None)(embed_layer1)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv1 = Activation('relu')(conv3)\n",
    "    conv3 = Conv1D(filters=num_filter, kernel_size=3, strides=1, padding='same', activation=None)(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3_pool = MaxPooling1D(pool_size=2, strides=None, padding='same')(conv3)\n",
    "    \n",
    "    conv4 = Conv1D(filters=num_filter, kernel_size=4, strides=1, padding='same', activation=None)(embed_layer1)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4 = Conv1D(filters=num_filter, kernel_size=4, strides=1, padding='same', activation=None)(conv4)\n",
    "    conv4 = BatchNormalization()(conv1)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4_pool = MaxPooling1D(pool_size=2, strides=None, padding='same')(conv4)\n",
    "    \n",
    "    conv5 = Conv1D(filters=num_filter, kernel_size=5, strides=1, padding='same', activation=None)(embed_layer1)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv1D(filters=num_filter, kernel_size=5, strides=1, padding='same', activation=None)(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5_pool = MaxPooling1D(pool_size=2, strides=None, padding='same')(conv5)\n",
    "    \n",
    "    conv_concat = Concatenate(axis=1)([conv1_pool, conv2_pool, conv3_pool, conv4_pool, conv5_pool])\n",
    "    \n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(conv_concat)\n",
    "    max_pool = GlobalMaxPooling1D()(conv_concat)\n",
    "    conv_concat = Concatenate()([avg_pool, max_pool])\n",
    "    # conv_concat = Flatten()(conv_concat)\n",
    "    conv_concat = Dropout(drop)(conv_concat)\n",
    "    conv_concat = Dense(300)(conv_concat)\n",
    "    conv_bn = BatchNormalization()(conv_concat)\n",
    "    conv_relu = Activation('relu')(conv_bn)\n",
    "    # conv_dropout = Dropout(drop)(conv_relu)\n",
    "    outputs1 = Dense(30, activation='sigmoid', name='textCNN')(conv_relu)\n",
    " \n",
    "\n",
    "    \"\"\"***************************************bi_gru********************************************************\"\"\"\n",
    "    gru_units= 64 # 100\n",
    " \n",
    "    embed_layer2 = Embedding(len(word2index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                            input_length=maxlen, trainable=True)(inputs)\n",
    "    \"\"\"x = LSTM(output_dim=100,activation='relu',inner_activation='relu', return_sequences=True)(x)\"\"\"\n",
    "    x1 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                          return_sequences=True))(embed_layer2)\n",
    "    x1 = attention_3d_block(x1)\n",
    "    x2 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                      return_sequences=True))(embed_layer2) \n",
    "    \n",
    "    x2 = attention_3d_block(x2)\n",
    "    x3 = Concatenate(axis=1)([x1, x2])\n",
    "    avg_pool = GlobalAveragePooling1D()(x3)\n",
    "    max_pool = GlobalMaxPooling1D()(x3)\n",
    "    # print(avg_pool.shape, max_pool.shape)\n",
    "    x5 = Concatenate(axis=1)([avg_pool, max_pool])\n",
    "    \n",
    "    fc = Dense(300)(x5)\n",
    "    bn = BatchNormalization()(fc)\n",
    "    bn = Activation('relu')(bn)\n",
    "    bn_dropout = Dropout(drop)(bn)\n",
    "    # bn_dropout = Flatten()(bn_dropout)\n",
    "    outputs2 = Dense(30, activation='sigmoid', name='bi_gru')(bn_dropout)\n",
    "    \n",
    "    \"\"\"******************************************hi_bi_gru*****************************************************\"\"\"\n",
    "    drop = 0.55\n",
    "    embed_layer3= Embedding(len(word2index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                            input_length=maxlen, trainable=True)(inputs)\n",
    "    x1 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                          return_sequences=True))(embed_layer3)\n",
    "    x1 = attention_3d_block(x1)\n",
    "    x2 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                      return_sequences=True))(embed_layer3)\n",
    "    x2 = attention_3d_block(x2)\n",
    "    x3= Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                      return_sequences=True))(x2)\n",
    "    x3 = attention_3d_block(x3)\n",
    "    \"\"\"\n",
    "    x3 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                       return_sequences=True))(x)\n",
    "    x4 = Concatenate(axis=1)([x1, x2, x3])\n",
    "    # x = Dense(200, activation='relu')(x)\n",
    "    x4 = Dropout(drop)(x4)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x5 = Concatenate(axis=1)([avg_pool, max_pool])\n",
    "    \"\"\"\n",
    "    x4 = Concatenate(axis=1)([x1, x3])\n",
    "    avg_pool = GlobalAveragePooling1D()(x4)\n",
    "    max_pool = GlobalMaxPooling1D()(x4)\n",
    "    x5 = Concatenate(axis=1)([avg_pool, max_pool])\n",
    "    fc = Dense(300)(x5)\n",
    "    bn = BatchNormalization()(fc)\n",
    "    bn = Activation('relu')(bn)\n",
    "    bn_dropout = Dropout(drop)(bn)\n",
    "    # bn_dropout = Flatten()(bn_dropout)\n",
    "    outputs3 = Dense(30, activation='sigmoid', name='hi_bi_gru')(bn_dropout)\n",
    "\n",
    "    \"\"\"******************************************capsule*****************************************************\"\"\"\n",
    "    embed_layer4= Embedding(len(word2index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                            input_length=maxlen, trainable=True)(inputs)\n",
    "    x = Bidirectional(GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p,\n",
    "                         return_sequences=True))(embed_layer4)\n",
    "    # capsule是一个卷积网络\n",
    "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n",
    "                      share_weights=True)(x)\n",
    "    # output_capsule = Lambda(lambda x: K.sqrt(K.sum(K.square(x), 2)))(capsule)\n",
    "    avg_pool = GlobalAveragePooling1D()(capsule)\n",
    "    max_pool = GlobalMaxPooling1D()(capsule)\n",
    "    capsule_concat  = Concatenate()([avg_pool, max_pool])\n",
    "    \n",
    "    capsule = Flatten()(capsule)\n",
    "    caspule = BatchNormalization()(capsule_concat)\n",
    "    capsule = Activation('relu')(capsule)\n",
    "    \n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "    outputs4 = Dense(30, activation='sigmoid', name='capsule')(capsule)\n",
    "    # now we get outputs1, outputs2, outputs3, outputs4\n",
    "    # outputs = Add()([outputs1, outputs2, outputs3, outputs4])\n",
    "    output_concat = Concatenate(axis=1)([outputs1, outputs2, outputs3, outputs4])\n",
    "    outputs = Dropout(drop)(output_concat)\n",
    "    outputs = Dense(30, activation='sigmoid')(outputs)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    learning_rate = 0.00001\n",
    "    adam = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0)\n",
    "    sgd = SGD(lr=learning_rate, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    rmsprop = RMSprop(lr=learning_rate, rho=0.9, epsilon=None, decay=0.0)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[f1_score])  # 先'adam'\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29828 samples, validate on 12784 samples\n",
      "Epoch 1/50\n",
      "29828/29828 [==============================] - 937s 31ms/step - loss: 0.1326 - f1_score: 0.2622 - val_loss: 0.1139 - val_f1_score: 0.3768\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.37680, saving model to ../models/weights/multi_model-dataArg_01-0.3768-0.1139.hdf5\n",
      "Epoch 2/50\n",
      "29828/29828 [==============================] - 1109s 37ms/step - loss: 0.1313 - f1_score: 0.2717 - val_loss: 0.1131 - val_f1_score: 0.3793\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.37680 to 0.37932, saving model to ../models/weights/multi_model-dataArg_02-0.3793-0.1131.hdf5\n",
      "Epoch 3/50\n",
      "16192/29828 [===============>..............] - ETA: 7:50 - loss: 0.1297 - f1_score: 0.2760"
     ]
    }
   ],
   "source": [
    "# for i in range(5):\n",
    "filepath=\"../models/weights/multi_model-dataArg_{epoch:02d}-{val_f1_score:.4f}-{val_loss:.4f}.hdf5\"\n",
    "\n",
    "# filepath=\"../models/weights/multi_model-independent-{epoch:02d}-{val_f1_score:.4f}-{val_loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\n",
    "# model = multi_model()\n",
    "model = multi_model_independent_embedding()\n",
    "model.load_weights('../models/weights/multi_model-dataArg_25-0.3667-0.1147.hdf5')\n",
    "model.fit(X_trains, y_trains, epochs=50, batch_size=32, validation_split=0.33, shuffle=True, \n",
    "          callbacks=[checkpoint]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  multi_model()\n",
    "model.load_weights('../models/weights/1103-multi_model-4models-36-0.6405-0.0679.hdf5')\n",
    "pred4 = model.predict(X_test, batch_size=1024))\n",
    "res, res_df = pred2res(pred4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
