{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/liujiaqi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://github.com/312shan/Subject-and-Sentiment-Analysis\n",
    "\"\"\"\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "from capsule import *\n",
    "import jieba\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, SpatialDropout2D, Activation, Embedding, Flatten, Conv2D, MaxPool2D, Lambda\n",
    " \n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Reshape, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from keras.layers import BatchNormalization, Permute\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "test_size = 0#.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.369 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "if test_size == 0:\n",
    "    stop_words = load_stop_words()\n",
    "    seqs, seqs_dev, word2index, y_train = raw_file2matrix(train_file, test_file, stop_words)\n",
    "    embeddings_index = load_embeddings_index()\n",
    "    embedding_matrix = get_embedding_matrix(word2index, embeddings_index)  # word-index与embedding的关系 \n",
    "    \n",
    "    X_train = np.loadtxt('../data/output/matrixes/X_train_2')\n",
    "    y_train = np.loadtxt('../data/output/matrixes/y_train_2')\n",
    "    X_test = np.loadtxt('../data/output/matrixes/X_test_2')\n",
    "    X_trains, y_trains = generate_shuffle_array(X_train, y_train)\n",
    "    drop_array(X_trains)\n",
    "else:\n",
    "    stop_words = load_stop_words()\n",
    "    seqs_train, seqs_valid, seqs_dev, word2index, y_train, y_valid, train_id_label_dict, valid_label = raw_file_2_matrix(train_file, test_file, stop_words, test_size=test_size)\n",
    "\n",
    "    embeddings_index = load_embeddings_index()\n",
    "    embedding_matrix = get_embedding_matrix(word2index, embeddings_index)  # word-index-embedding是它们之间的链接关系\n",
    "    X_train, X_valid, X_test = get_padding_data(seqs_train, seqs_valid, seqs_dev)  # seqs needs to be a list of a list.把列表变成矩阵，列数是embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine import Layer, InputSpec\n",
    "from keras.layers import Flatten\n",
    "import tensorflow as tf\n",
    "def attention_3d_block(inputs):\n",
    "    # https://github.com/keras-team/keras/issues/1472\n",
    "    # https://github.com/philipperemy/keras-attention-mechanism\n",
    "    TIME_STEPS = 20\n",
    "    INPUT_DIM = 2\n",
    "    # if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
    "    SINGLE_ATTENTION_VECTOR = False\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    # print(a.shape, input_dim, TIME_STEPS)\n",
    "    # a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    # a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    # print(inputs.shape, a_probs.shape)\n",
    "    # output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    output_attention_mul = Concatenate(axis=1)([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "    \"\"\"\n",
    "    # https://stackoverflow.com/questions/42918446/how-to-add-an-attention-mechanism-in-keras\n",
    "    activations = LSTM(units, return_sequences=True)(embedded)\n",
    "\n",
    "    # compute importance for each step\n",
    "    attention = Dense(1, activation='tanh')(activations)\n",
    "    attention = Flatten()(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = RepeatVector(units)(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "\n",
    "    sent_representation = merge([activations, attention], mode='mul')\n",
    "    \"\"\"\n",
    "def model_attention_applied_after_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    lstm_units = 32\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "    \n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model\n",
    "\n",
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    TensorFlow backend.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        self.k = k\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], (input_shape[2] * self.k))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # swap last two dimensions since top_k will be applied along the last dimension\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "        \n",
    "        # extract top_k, returns two tensors [values, indices]\n",
    "        top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0]\n",
    "        \n",
    "        # return flattened output\n",
    "        # return Flatten()(top_k)\n",
    "        return top_k\n",
    "\n",
    "def RCNN_model():\n",
    "    drop = 0.5\n",
    "    learning_rate = 0.005  # 0.0001\n",
    "    maxlen = 100\n",
    "    num_filter = 128\n",
    "    gru_units = 150\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    embed_layer = Embedding(len(word2index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                            input_length=maxlen, trainable=True)(inputs)\n",
    "    embed_layer = SpatialDropout1D(drop)(embed_layer)\n",
    "    \n",
    "    # 1\n",
    "    bi_lstm_1 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                          return_sequences=True))(embed_layer)\n",
    "    # bi_lstm_1 = attention_3d_block(bi_lstm_1)\n",
    "    bi_lstm_2 = Bidirectional(GRU(gru_units, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, \n",
    "                          return_sequences=True))(bi_lstm_1)\n",
    "    # bi_lstm_2 = attention_3d_block(bi_lstm_2)\n",
    "    # 2\n",
    "    concat = Concatenate(axis=1)([bi_lstm_2, embed_layer])\n",
    "    # pool = Lambda(lambda x: tf.reshape(tf.nn.top_k(tf.transpose(x,[0,2,1]),k=2)[0],shape=[-1,6]))(concat)\n",
    "    # pool = KMaxPooling(k=concat.shape[2])(concat)\n",
    "    # pool = kmaxpooling.call(concat)\n",
    "    pool = MaxPooling1D(pool_size=2, strides=2, padding='valid')(concat)\n",
    "\n",
    "    # print(concat.shape, pool.shape)  # (?, ?, 300) (?, ?, 300) 128\n",
    "    # pool = Reshape((-1, 300))(pool)\n",
    "    print(concat.shape, pool.shape)\n",
    "    conv = Conv1D(filters=num_filter, kernel_size=2, strides=1, padding='same', activation='relu')(pool)\n",
    "    # conv_flatten = Flatten()(pool)\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(conv)\n",
    "    max_pool = GlobalMaxPooling1D()(conv)\n",
    "    conv_concat  = Concatenate()([avg_pool, max_pool])\n",
    "    \n",
    "    conv_bn = BatchNormalization()(conv_concat)\n",
    "    conv_relu = Activation('relu')(conv_bn)\n",
    "    conv_drop = Dropout(drop)(conv_relu)\n",
    "    outputs = Dense(30, activation=\"sigmoid\")(conv_drop)\n",
    "    \n",
    "    # print(inputs.shape, outputs.shape)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_score])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.60\n",
    "print(X_train.shape, y_train.shape)\n",
    "for i in range(5):\n",
    "    # log_filepath = './tmp/log'\n",
    "    filepath=\"../models/weights/rcnn_weight-dataArg-{epoch:02d}-{val_f1_score:.4f}-{val_loss:.4f}.hdf5\"\n",
    "    # callback = keras.callbacks.TensorBoard(log_dir=log_filepath, write_images=1, histogram_freq=1)\n",
    "    \n",
    "    # early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_f1_score', verbose=1, save_best_only=True, \n",
    "                                 save_weights_only=True, mode='max')\n",
    "    model = RCNN_model()\n",
    "    \n",
    "    model.fit(X_train, y_train, batch_size=16, shuffle=True, epochs=40, verbose=1, \n",
    "              callbacks=[checkpoint], \n",
    "              validation_split=0.33) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
